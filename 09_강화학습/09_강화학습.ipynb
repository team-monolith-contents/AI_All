{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"-C2lKQoVuBHo"},"source":["# **[9] 강화학습**\n","\n","**[학습 목표]**\n","1. 강화학습의 학습 원리를 이해할 수 있다.\n","2. 강화학습이 사용되는 다양한 사례를 탐색할 수 있다.\n","3. 강화학습을 이용하여 실제 게임을 학습시킬 수 있다."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2-PfUmkV5nKP"},"source":["## **1. 강화학습**  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2sJRHyaQ-TCl"},"source":["### **(1) 강화학습의 원리**  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OPQhJ4HYuBHu"},"source":["강화학습은 **시행착오**를 통해 주어진 상태에서 최적의 행동을 선택하는 학습 방법이에요.  \n","강화학습에서는 지도/비지도 기계학습과 달리, 아무런 데이터가 주어지지 않아요.  \n","대신 주어진 상태에서 행동의 결과에 대한 **보상**을 에이전트에게 주어요.   \n","에이전트는 행동과 보상에 대한 정보를 바탕으로, 어떤 행동을 취해야 가장 큰 보상을 가져올지 스스로 학습하는 것이지요."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"O3kZwpNLuBHu"},"source":["게임을 하는 사람이 게임 실력을 키우는 과정을 통해, 강화 학습의 원리를 자세하게 이해해보아요.\n","\n","<table>\n","<tr>\n","    <th><img src=\"https://tmn-bucket-materials-all.s3.ap-northeast-2.amazonaws.com/image/ai/AI-09-01.png\" width=\"750\"></th>\n","    </tr>\n","<tr>\n","    <th>강화학습의 원리</th>\n","</tr>\n","</table>  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kkMfDlFJuBHu"},"source":["에이전트는 둘러싸고 있는 환경과 상호작용하는 행동의 주체를 의미해요.  \n","환경은 에이전트를 둘러싸고 있는 것들이에요.  \n","상태는 에이전트에 대한 상태를 숫자로 표현한 것이에요.  \n","행동은 에이전트가 실제 행동한 내용이에요.  \n","보상은 에이전트가 행동한 결과로 받게 되는 것이에요.  \n","정책은 행동에 따른 보상을 기반으로 에이전트가 행동하는 방향이에요.  \n","\n","> 에이전트는 게임이라는 환경에서, 어떻게 행동해야 더 많은 보상을 얻을지 학습해요.   \n","> 학습을 통해, 에이전트는 더 많은 보상을 얻기 위한 정책을 수립하는 것이지요."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EJmN5WE_1Ev9"},"source":["### **(2) 강화학습과 지도/비지도학습**  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WSk9LaRF1Ev9"},"source":["지난 4단원에서 인공지능을 활용 목적에 따라 지도학습, 비지도학습, 강화학습으로 나누었었어요.  \n","지도학습은 회귀/분류를 위해, 비지도학습은 그룹화를 위해, 강화학습은 더 좋은 선택을 하기 위해 사용한다고 했었어요.  \n","\n","<table>\n","<tr>\n","    <th><img src=\"https://tmn-bucket-materials-all.s3.ap-northeast-2.amazonaws.com/image/ai/AI-09-02.png\" width=\"750\"></th>\n","    </tr>\n","<tr>\n","    <th>강화학습과 지도/비지도학습</th>\n","</tr>\n","</table>  \n","\n","강화학습은 활용 목적이 지도/비지도학습과 크게 다른 만큼, 학습 형태도 다른 부분이 있어요.  \n","우선, 지도/비지도학습이 데이터가 이미 주어진 **정적(static)** 환경에서 학습이 진행되었다면,  \n","강화학습은 데이터 없이 보상을 얻으면서 학습하는 **동적(dynamic)** 환경에서 진행되어요.   \n","강화학습은 동적 상태에서 데이터를 수집하는 과정까지 함께 포함된 알고리즘이에요.  \n","\n","강화학습은 데이터 없이 보상만을 가지고 학습하기 때문에,  \n","학습시키는 게임의 규칙을 모르는 상황에서도 학습할 수 있어요.  \n","그렇기 때문에 복잡한 상황에서 최선의 동작을 찾고자 할 때 강화학습이 많이 사용되어요.  \n","\n","<table>\n","<tr>\n","    <th><img src=\"https://tmn-bucket-materials-all.s3.ap-northeast-2.amazonaws.com/image/ai/AI-09-03.png\" width=\"750\"></th>\n","    </tr>\n","<tr>\n","    <th>강화학습의 특징</th>\n","</tr>\n","</table>  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T95F0kWMuBHv"},"source":["### **(3) 강화학습의 생활 속 사례**  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uTPyEUeTuBHv"},"source":["강화학습은 실제 생활 속에서 다양하게 사용될 수 있어요.  \n","아래 두 가지 예시를 소개하지만, 그 외의 다른 예시들도 유튜버 \"생활코딩\"님께서 정리하신\n","<a href=\"https://docs.google.com/spreadsheets/d/1TnRTcrA7yapqfnXCm_M6ZURD7ofLbH62mZIjznwEOKI/edit#gid=414903589\">링크</a>\n","를 통해 확인할 수 있어요.  \n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Nwp9BpAtuBHv"},"source":["예시 1: 인공지능 골키퍼가 상대 선수의 축구공을 막는다.   \n","\n","<a href=\"https://youtu.be/7Yc6ZHixgRk\"><button style=\"width:100px; height:50px\">링크</button></a>\n","\n","- 에이전트 : 골키퍼\n","- 환경 : 축구장\n","- 행동 : 상대가 찬 공을 막기 위해 적절한 위치로 이동한다.\n","- 보상 : 상대가 찬 공이 골대에 들어가지 않으면 득점을 막을 수 있다.\n","- 상태 : 상대가 차는 축구공을 막기 위한 골대의 골키퍼"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X4WRR0JYuBHw"},"source":["예시 2: 강화학습을 통해 볼링공이 볼링을 맞춥니다.   \n","\n","<a href=\"https://www.youtube.com/watch?v=nReMgotclXU\"><button style=\"width:100px; height:50px\">링크</button></a>\n","\n","- 에이전트 : 볼링공\n","- 환경 : 볼링장\n","- 행동 : 볼링공의 위치, 스핀을 달리하여 굴리기\n","- 보상 : 볼링핀을 시간 내에 맞추면 상을 준다. 못 맞히거나 시간을 초과하면 벌을 준다.\n","- 상태 : 볼링공과 볼링핀의 거리, 볼링공의 위치"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q8_OIzBpuBHw"},"source":["## **2. 강화학습 실습**  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7mDLmcfgYSjf"},"source":["### **(1) CartPole 플레이**  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FYUKSmRC1Ev_"},"source":["우선 CartPole이 어떤 게임인지 알아야겠죠?  \n","말 그대로 Cart를 움직여 Pole이 떨어지지 않게 하는 게임이에요.    \n","직접 플레이해보고 최고점을 누가 기록했는지 대결해보아요!  \n","\n","<a href=\"https://jeffjar.me/cartpole.html\"><button style=\"width:100px; height:50px\">플레이</button></a>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"29YfXikp1Ev_"},"source":["**[문제1] CartPole에서 강화학습의 각 요소는 무엇에 해당되는지 작성해보아요.**"]},{"cell_type":"raw","metadata":{"id":"74SrHfop1Ev_"},"source":[" 👉\n"," 👉"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ju8aeEb21Ev_"},"source":["**[문제2] CartPole을 플레이하고, 각자 고득점을 얻기 위해 어떤 방법을 사용하였는지 이야기해보아요.**"]},{"cell_type":"raw","metadata":{"id":"Og78j16V1Ev_"},"source":[" 👉\n"," 👉"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5v0Kf1t71Ev_"},"source":["### **(2) CartPole 학습**  \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jZLbQcuV1Ev_"},"source":["CartPole은 강화학습을 사용하여 훈련시킬 수 있는 대표적인 게임 중 하나에요.  \n","저희가 제공하는 강화학습 모델을 사용하여 학습이 이루어지는 과정을 관찰해보아요."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7103,"status":"ok","timestamp":1688053560332,"user":{"displayName":"성무열","userId":"14501712316857642086"},"user_tz":-540},"id":"ihU831FD1MMF","outputId":"f600c514-0c7c-4401-d09f-1c9b44ba9332"},"outputs":[],"source":["# stable-baselines3 라이브러리를 사용하여 강화학습 모델을 간편하게 사용\n","!pip install stable-baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12850,"status":"ok","timestamp":1688053575536,"user":{"displayName":"성무열","userId":"14501712316857642086"},"user_tz":-540},"id":"j3kStlnV1Ev_"},"outputs":[],"source":["import stable_baselines3\n","import gymnasium as gym\n","import numpy as np\n","\n","from stable_baselines3 import PPO\n","from stable_baselines3.ppo import MlpPolicy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"elapsed":80169,"status":"error","timestamp":1688053658461,"user":{"displayName":"성무열","userId":"14501712316857642086"},"user_tz":-540},"id":"7hGutF081EwA","outputId":"540a6b61-9aa6-4d65-ff7a-d9f327788f36"},"outputs":[],"source":["# 학습 진행(총 4가지 버전 저장)\n","env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","model = PPO(MlpPolicy, env, verbose=0)\n","\n","TIMESTEPS = 1000\n","for iters in range(100 + 1):\n","    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n","    if iters in [10, 20, 50, 100]:\n","        model.save(f\"{iters}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5101,"status":"ok","timestamp":1688054116822,"user":{"displayName":"성무열","userId":"14501712316857642086"},"user_tz":-540},"id":"AQMmABbH1EwA"},"outputs":[],"source":["env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","\n","# 이름 수정하여 다른 모델도 불러올 수 있음\n","model_name = \"10\"\n","model = PPO.load(model_name, env=env)\n","\n","# 해당 모델로 수행하는 결과 렌더링\n","# jupyter에서는 새로운 창 띄워지지 않음. gnwrapper나 다른 방법을 사용해야 함\n","\n","obs, _ = env.reset()\n","for i in range(1000):\n","    action, _state = model.predict(obs, deterministic=True)\n","    obs, reward, done, info, _ = env.step(action)\n","    env.render()\n","    if done:\n","        obs, _ = env.reset()\n","\n","env.close()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"kaggle","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8c352b738e5f22da9f29eb9cb9994f25c5223ab3395af3650b8321ab644afce4"}}},"nbformat":4,"nbformat_minor":0}
